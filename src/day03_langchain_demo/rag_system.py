#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Day 3: LangChainåŸºç¡€ä¸RAGç³»ç»Ÿæ„å»º
å®ç°å®Œæ•´çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç³»ç»Ÿ
"""

import os
import sys
import time
from typing import List, Dict, Any, Optional
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'src' / 'day01_embedding_demo'))
sys.path.insert(0, str(project_root / 'src' / 'day02_faiss_demo'))

# å¯¼å…¥å‰é¢çš„æ¨¡å—
from basic_embedding import EmbeddingDemo
from vector_database import VectorDatabaseDemo

# LangChainå¯¼å…¥
try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain.schema import Document
    from langchain.prompts import PromptTemplate
    from langchain.chains import LLMChain
    LANGCHAIN_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ LangChainå¯¼å…¥å¤±è´¥: {e}")
    LANGCHAIN_AVAILABLE = False

# LLMåŸºç±»å¯¼å…¥
try:
    from langchain_community.llms.base import LLM
    LLM_BASE_AVAILABLE = True
except ImportError:
    try:
        from langchain.llms.base import LLM
        LLM_BASE_AVAILABLE = True
    except ImportError:
        print("âš ï¸ LLMåŸºç±»ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–å®ç°")
        LLM_BASE_AVAILABLE = False
        
        # åˆ›å»ºç®€åŒ–çš„LLMåŸºç±»
        class LLM:
            def __init__(self):
                pass
            
            @property
            def _llm_type(self) -> str:
                return "base"
            
            def _call(self, prompt: str, stop=None) -> str:
                return "Mock response"

# é€šä¹‰åƒé—®APIå¯¼å…¥
try:
    import dashscope
    from dashscope import Generation
    DASHSCOPE_AVAILABLE = True
except ImportError:
    print("âš ï¸ DashScopeæœªå®‰è£…ï¼Œå°†ä½¿ç”¨Mock LLM")
    DASHSCOPE_AVAILABLE = False


class QwenLLM(LLM):
    """é€šä¹‰åƒé—®LLMåŒ…è£…å™¨ï¼Œå…¼å®¹LangChainæ¥å£"""
    
    def __init__(self, api_key: str = None, model: str = "qwen-turbo"):
        super().__init__()
        # ä½¿ç”¨object.__setattr__é¿å…PydanticéªŒè¯é—®é¢˜
        object.__setattr__(self, 'api_key', api_key or os.getenv('DASHSCOPE_API_KEY'))
        object.__setattr__(self, 'model', model)
        
        if self.api_key and DASHSCOPE_AVAILABLE:
            dashscope.api_key = self.api_key
            object.__setattr__(self, 'available', True)
        else:
            object.__setattr__(self, 'available', False)
            print("âš ï¸ é€šä¹‰åƒé—®APIä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨Mockå“åº”")
    
    @property
    def _llm_type(self) -> str:
        return "qwen"
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        """è°ƒç”¨é€šä¹‰åƒé—®APIç”Ÿæˆå›ç­”"""
        if not self.available:
            return self._mock_response(prompt)
        
        try:
            response = Generation.call(
                model=self.model,
                prompt=prompt,
                max_tokens=1000,
                temperature=0.7
            )
            
            if response.status_code == 200:
                return response.output.text.strip()
            else:
                print(f"âŒ APIè°ƒç”¨å¤±è´¥: {response.message}")
                return self._mock_response(prompt)
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}")
            return self._mock_response(prompt)
    
    def _mock_response(self, prompt: str) -> str:
        """Mockå“åº”ï¼Œç”¨äºAPIä¸å¯ç”¨æ—¶"""
        if "ä»€ä¹ˆæ˜¯" in prompt or "ä»‹ç»" in prompt:
            return "è¿™æ˜¯ä¸€ä¸ªå…³äºæ‚¨è¯¢é—®ä¸»é¢˜çš„è¯¦ç»†ä»‹ç»ã€‚ç”±äºAPIé™åˆ¶ï¼Œè¿™é‡Œæ˜¾ç¤ºçš„æ˜¯æ¨¡æ‹Ÿå›ç­”ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šæ˜¾ç¤ºé€šä¹‰åƒé—®çš„æ™ºèƒ½å›ç­”ã€‚"
        elif "å¦‚ä½•" in prompt or "æ€ä¹ˆ" in prompt:
            return "ä»¥ä¸‹æ˜¯è§£å†³æ‚¨é—®é¢˜çš„æ­¥éª¤å’Œæ–¹æ³•ã€‚è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå›ç­”ï¼Œå®é™…ä½¿ç”¨æ—¶ä¼šè°ƒç”¨é€šä¹‰åƒé—®APIç”Ÿæˆæ›´å‡†ç¡®çš„å›ç­”ã€‚"
        else:
            return "åŸºäºæ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ï¼Œè¿™é‡Œæ˜¯å¯¹æ‚¨é—®é¢˜çš„å›ç­”ã€‚è¿™æ˜¯æ¨¡æ‹Ÿå›ç­”ï¼Œå®é™…ä¼šä½¿ç”¨é€šä¹‰åƒé—®APIç”Ÿæˆæ™ºèƒ½å›ç­”ã€‚"


class RAGSystem:
    """å®Œæ•´çš„RAG(æ£€ç´¢å¢å¼ºç”Ÿæˆ)ç³»ç»Ÿ"""
    
    def __init__(self, persist_directory: str = "./rag_db"):
        """åˆå§‹åŒ–RAGç³»ç»Ÿ"""
        self.persist_directory = persist_directory
        
        # åˆå§‹åŒ–ç»„ä»¶
        print("ğŸš€ åˆå§‹åŒ–RAGç³»ç»Ÿ...")
        
        # 1. åˆå§‹åŒ–Embeddingæ¨¡å—
        self.embedding_demo = EmbeddingDemo()
        print("âœ… Embeddingæ¨¡å—åˆå§‹åŒ–å®Œæˆ")
        
        # 2. åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.vector_db = VectorDatabaseDemo(persist_directory=persist_directory)
        print("âœ… å‘é‡æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
        
        # 3. åˆå§‹åŒ–LLM
        self.llm = QwenLLM()
        print("âœ… LLMåˆå§‹åŒ–å®Œæˆ")
        
        # 4. åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        if LANGCHAIN_AVAILABLE:
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=50,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ";", "ï¼Œ", " ", ""]
            )
            print("âœ… æ–‡æœ¬åˆ†å‰²å™¨åˆå§‹åŒ–å®Œæˆ")
        else:
            self.text_splitter = None
            print("âš ï¸ LangChainä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•åˆ†å‰²")
        
        # 5. åˆå§‹åŒ–æç¤ºæ¨¡æ¿
        self.prompt_template = PromptTemplate(
            input_variables=["context", "question"],
            template="""
åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•ä»æä¾›çš„ä¿¡æ¯ä¸­æ‰¾åˆ°ç­”æ¡ˆã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†å’Œå‡†ç¡®çš„å›ç­”ï¼š
"""
        )
        
        print("ğŸ‰ RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
    
    def split_text(self, text: str) -> List[str]:
        """åˆ†å‰²æ–‡æœ¬ä¸ºå°å—"""
        if self.text_splitter:
            # ä½¿ç”¨LangChainçš„æ–‡æœ¬åˆ†å‰²å™¨
            docs = self.text_splitter.split_text(text)
            return docs
        else:
            # ç®€å•åˆ†å‰²
            sentences = text.replace('ã€‚', 'ã€‚\n').replace('ï¼', 'ï¼\n').replace('ï¼Ÿ', 'ï¼Ÿ\n').split('\n')
            chunks = []
            current_chunk = ""
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                    
                if len(current_chunk + sentence) < 500:
                    current_chunk += sentence
                else:
                    if current_chunk:
                        chunks.append(current_chunk)
                    current_chunk = sentence
            
            if current_chunk:
                chunks.append(current_chunk)
            
            return chunks
    
    def add_documents(self, texts: List[str], metadatas: List[Dict] = None) -> List[str]:
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        print(f"ğŸ“š å¼€å§‹å¤„ç† {len(texts)} ä¸ªæ–‡æ¡£...")
        
        all_chunks = []
        all_metadatas = []
        
        for i, text in enumerate(texts):
            # åˆ†å‰²æ–‡æœ¬
            chunks = self.split_text(text)
            all_chunks.extend(chunks)
            
            # ä¸ºæ¯ä¸ªchunkæ·»åŠ å…ƒæ•°æ®
            base_metadata = metadatas[i] if metadatas and i < len(metadatas) else {}
            for j, chunk in enumerate(chunks):
                chunk_metadata = base_metadata.copy()
                chunk_metadata.update({
                    'doc_id': i,
                    'chunk_id': j,
                    'chunk_count': len(chunks),
                    'original_length': len(text),
                    'chunk_length': len(chunk)
                })
                all_metadatas.append(chunk_metadata)
        
        print(f"ğŸ“„ æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…±ç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æœ¬å—")
        
        # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
        doc_ids = self.vector_db.add_documents(all_chunks, all_metadatas)
        
        print(f"âœ… æˆåŠŸæ·»åŠ  {len(doc_ids)} ä¸ªæ–‡æ¡£å—åˆ°çŸ¥è¯†åº“")
        return doc_ids
    
    def retrieve_documents(self, query: str, top_k: int = 3) -> List[Dict]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        print(f"ğŸ” æ£€ç´¢æŸ¥è¯¢: '{query}'")
        
        results = self.vector_db.search_similar(query, n_results=top_k)
        
        print(f"ğŸ“‹ æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
        for i, result in enumerate(results, 1):
            print(f"  {i}. ç›¸ä¼¼åº¦: {result['similarity']:.3f} | é•¿åº¦: {len(result['document'])} å­—ç¬¦")
        
        return results
    
    def generate_answer(self, query: str, context_docs: List[Dict]) -> str:
        """åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆå›ç­”"""
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for i, doc in enumerate(context_docs, 1):
            context_parts.append(f"æ–‡æ¡£{i}: {doc['document']}")
        
        context = "\n\n".join(context_parts)
        
        # ç”Ÿæˆæç¤º
        prompt = self.prompt_template.format(context=context, question=query)
        
        print("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›ç­”...")
        
        # è°ƒç”¨LLMç”Ÿæˆå›ç­”
        answer = self.llm._call(prompt)
        
        return answer
    
    def query(self, question: str, top_k: int = 3) -> Dict[str, Any]:
        """å®Œæ•´çš„RAGæŸ¥è¯¢æµç¨‹"""
        print(f"\nğŸ¯ RAGæŸ¥è¯¢: '{question}'")
        print("=" * 50)
        
        start_time = time.time()
        
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_docs = self.retrieve_documents(question, top_k=top_k)
        
        if not retrieved_docs:
            return {
                'question': question,
                'answer': 'æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚',
                'retrieved_docs': [],
                'retrieval_time': time.time() - start_time,
                'generation_time': 0,
                'total_time': time.time() - start_time
            }
        
        retrieval_time = time.time() - start_time
        
        # 2. ç”Ÿæˆå›ç­”
        generation_start = time.time()
        answer = self.generate_answer(question, retrieved_docs)
        generation_time = time.time() - generation_start
        
        total_time = time.time() - start_time
        
        result = {
            'question': question,
            'answer': answer,
            'retrieved_docs': retrieved_docs,
            'retrieval_time': retrieval_time,
            'generation_time': generation_time,
            'total_time': total_time
        }
        
        print(f"\nğŸ“Š æŸ¥è¯¢ç»Ÿè®¡:")
        print(f"  æ£€ç´¢æ—¶é—´: {retrieval_time:.3f}ç§’")
        print(f"  ç”Ÿæˆæ—¶é—´: {generation_time:.3f}ç§’")
        print(f"  æ€»æ—¶é—´: {total_time:.3f}ç§’")
        
        return result
    
    def get_knowledge_base_stats(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        return self.vector_db.get_collection_stats()
    
    def clear_knowledge_base(self):
        """æ¸…ç©ºçŸ¥è¯†åº“"""
        self.vector_db.delete_collection()
        print("ğŸ—‘ï¸ çŸ¥è¯†åº“å·²æ¸…ç©º")


def demo_rag_system():
    """RAGç³»ç»Ÿæ¼”ç¤º"""
    print("ğŸ‰ Day 3: LangChainåŸºç¡€ä¸RAGç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)
    
    # 1. åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag = RAGSystem(persist_directory="./day03_rag_db")
    
    # 2. å‡†å¤‡æµ‹è¯•æ–‡æ¡£
    test_documents = [
        """
        äººå·¥æ™ºèƒ½(AI)æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚
        AIåŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰å¤šä¸ªå­é¢†åŸŸã€‚
        æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼å’Œè§„å¾‹ã€‚
        æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ï¼Œåœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚
        """,
        """
        RAG(æ£€ç´¢å¢å¼ºç”Ÿæˆ)æ˜¯ä¸€ç§ç»“åˆäº†ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„AIæŠ€æœ¯ã€‚
        RAGç³»ç»Ÿé¦–å…ˆä»å¤§å‹çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œç„¶ååŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯ç”Ÿæˆå›ç­”ã€‚
        è¿™ç§æ–¹æ³•å¯ä»¥æä¾›æ›´å‡†ç¡®ã€æ›´å…·ä½“çš„å›ç­”ï¼Œå› ä¸ºå®ƒåŸºäºå®é™…çš„çŸ¥è¯†è€Œä¸æ˜¯ä»…ä»…ä¾èµ–æ¨¡å‹çš„å‚æ•°çŸ¥è¯†ã€‚
        RAGåœ¨é—®ç­”ç³»ç»Ÿã€èŠå¤©æœºå™¨äººã€çŸ¥è¯†ç®¡ç†ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚
        """,
        """
        å‘é‡æ•°æ®åº“æ˜¯ä¸“é—¨ç”¨äºå­˜å‚¨å’Œæ£€ç´¢é«˜ç»´å‘é‡çš„æ•°æ®åº“ç³»ç»Ÿã€‚
        åœ¨AIåº”ç”¨ä¸­ï¼Œæ–‡æœ¬ã€å›¾åƒç­‰æ•°æ®é€šå¸¸è¢«è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºï¼Œç„¶åå­˜å‚¨åœ¨å‘é‡æ•°æ®åº“ä¸­ã€‚
        å‘é‡æ•°æ®åº“æ”¯æŒé«˜æ•ˆçš„ç›¸ä¼¼æ€§æœç´¢ï¼Œå¯ä»¥å¿«é€Ÿæ‰¾åˆ°ä¸æŸ¥è¯¢å‘é‡æœ€ç›¸ä¼¼çš„å‘é‡ã€‚
        å¸¸è§çš„å‘é‡æ•°æ®åº“åŒ…æ‹¬Pineconeã€Weaviateã€ChromaDBã€FAISSç­‰ã€‚
        """,
        """
        LangChainæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºåŸºäºå¤§è¯­è¨€æ¨¡å‹åº”ç”¨çš„æ¡†æ¶ã€‚
        å®ƒæä¾›äº†ä¸°å¯Œçš„ç»„ä»¶å’Œå·¥å…·ï¼ŒåŒ…æ‹¬æ–‡æ¡£åŠ è½½å™¨ã€æ–‡æœ¬åˆ†å‰²å™¨ã€å‘é‡å­˜å‚¨ã€æ£€ç´¢å™¨ã€é“¾ç­‰ã€‚
        LangChainç®€åŒ–äº†RAGç³»ç»Ÿçš„æ„å»ºè¿‡ç¨‹ï¼Œè®©å¼€å‘è€…å¯ä»¥å¿«é€Ÿæ­å»ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿã€‚
        é€šè¿‡LangChainï¼Œå¯ä»¥è½»æ¾é›†æˆä¸åŒçš„LLMã€å‘é‡æ•°æ®åº“å’Œå…¶ä»–AIæœåŠ¡ã€‚
        """,
        """
        Embeddingæ˜¯å°†æ–‡æœ¬ã€å›¾åƒç­‰æ•°æ®è½¬æ¢ä¸ºæ•°å€¼å‘é‡çš„æŠ€æœ¯ã€‚
        å¥½çš„embeddingèƒ½å¤Ÿæ•æ‰æ•°æ®çš„è¯­ä¹‰ä¿¡æ¯ï¼Œä½¿å¾—è¯­ä¹‰ç›¸ä¼¼çš„æ•°æ®åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»è¾ƒè¿‘ã€‚
        å¸¸ç”¨çš„æ–‡æœ¬embeddingæ¨¡å‹åŒ…æ‹¬Word2Vecã€GloVeã€BERTã€OpenAIçš„text-embedding-ada-002ç­‰ã€‚
        åœ¨RAGç³»ç»Ÿä¸­ï¼Œembeddingç”¨äºå°†æ–‡æ¡£å’ŒæŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡ï¼Œä»¥ä¾¿è¿›è¡Œç›¸ä¼¼æ€§æœç´¢ã€‚
        """
    ]
    
    # æ–‡æ¡£å…ƒæ•°æ®
    metadatas = [
        {"topic": "äººå·¥æ™ºèƒ½åŸºç¡€", "category": "AIæ¦‚å¿µ", "difficulty": "åˆçº§"},
        {"topic": "RAGæŠ€æœ¯", "category": "AIåº”ç”¨", "difficulty": "ä¸­çº§"},
        {"topic": "å‘é‡æ•°æ®åº“", "category": "æ•°æ®å­˜å‚¨", "difficulty": "ä¸­çº§"},
        {"topic": "LangChainæ¡†æ¶", "category": "å¼€å‘å·¥å…·", "difficulty": "ä¸­çº§"},
        {"topic": "EmbeddingæŠ€æœ¯", "category": "AIæŠ€æœ¯", "difficulty": "ä¸­çº§"}
    ]
    
    print("\nğŸ“š 3. æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“")
    doc_ids = rag.add_documents(test_documents, metadatas)
    
    print("\nğŸ“Š 4. çŸ¥è¯†åº“ç»Ÿè®¡")
    stats = rag.get_knowledge_base_stats()
    print(f"  æ€»æ–‡æ¡£æ•°: {stats['total_documents']}")
    print(f"  æ•°æ®åº“ç±»å‹: {stats['database_type']}")
    
    print("\nğŸ” 5. RAGæŸ¥è¯¢æ¼”ç¤º")
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿ",
        "å‘é‡æ•°æ®åº“æœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ",
        "LangChainæ¡†æ¶çš„ä¸»è¦åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ",
        "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ çš„å…³ç³»ï¼Ÿ",
        "å¦‚ä½•é€‰æ‹©åˆé€‚çš„embeddingæ¨¡å‹ï¼Ÿ"
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n--- æŸ¥è¯¢ {i} ---")
        result = rag.query(query, top_k=2)
        
        print(f"\nğŸ’¬ é—®é¢˜: {result['question']}")
        print(f"ğŸ¤– å›ç­”: {result['answer']}")
        print(f"\nğŸ“‹ æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£:")
        for j, doc in enumerate(result['retrieved_docs'], 1):
            print(f"  {j}. [{doc['metadata'].get('topic', 'Unknown')}] ç›¸ä¼¼åº¦: {doc['similarity']:.3f}")
            print(f"     {doc['document'][:100]}...")
    
    print("\nğŸ¯ 6. æ€§èƒ½åˆ†æ")
    print("RAGç³»ç»Ÿå±•ç¤ºäº†ä»¥ä¸‹èƒ½åŠ›:")
    print("âœ… æ–‡æ¡£æ™ºèƒ½åˆ†å‰²å’Œå‘é‡åŒ–å­˜å‚¨")
    print("âœ… åŸºäºè¯­ä¹‰çš„æ–‡æ¡£æ£€ç´¢")
    print("âœ… ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å›ç­”ç”Ÿæˆ")
    print("âœ… ç«¯åˆ°ç«¯çš„é—®ç­”æµç¨‹")
    print("âœ… å¯æ‰©å±•çš„çŸ¥è¯†åº“ç®¡ç†")
    
    print("\nğŸ‰ Day 3 å­¦ä¹ å®Œæˆï¼")
    print("\nğŸ“š ä»Šæ—¥å­¦ä¹ è¦ç‚¹:")
    print("1. âœ… LangChainæ¡†æ¶çš„æ ¸å¿ƒç»„ä»¶å’Œä½¿ç”¨æ–¹æ³•")
    print("2. âœ… æ–‡æ¡£åŠ è½½ã€åˆ†å‰²å’Œå‘é‡åŒ–çš„å®Œæ•´æµç¨‹")
    print("3. âœ… æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)çš„å®ç°åŸç†")
    print("4. âœ… é›†æˆå‰ä¸¤å¤©çš„æˆæœæ„å»ºç«¯åˆ°ç«¯ç³»ç»Ÿ")
    print("5. âœ… æç¤ºå·¥ç¨‹å’Œä¸Šä¸‹æ–‡ç®¡ç†æŠ€å·§")
    
    print("\nğŸ”— ä¸‹ä¸€æ­¥å­¦ä¹ :")
    print("- Day 4: é«˜çº§RAGæŠ€æœ¯(é‡æ’åºã€å¤šè½®å¯¹è¯)")
    print("- Day 5: Agentç³»ç»Ÿæ„å»º")
    
    return rag


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    rag_system = demo_rag_system()