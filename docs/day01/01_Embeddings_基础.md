# 01_Embeddings_åŸºç¡€

## ğŸ“– çŸ¥è¯†ç‚¹æ¦‚å¿µ

**Embeddingï¼ˆåµŒå…¥/å‘é‡åŒ–ï¼‰** æ˜¯å°†æ–‡æœ¬ã€å›¾åƒç­‰éç»“æ„åŒ–æ•°æ®è½¬æ¢ä¸ºé«˜ç»´æ•°å€¼å‘é‡çš„æŠ€æœ¯ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œæ–‡æœ¬embeddingå°†è¯æ±‡ã€å¥å­æˆ–æ–‡æ¡£æ˜ å°„åˆ°è¿ç»­çš„å‘é‡ç©ºé—´ä¸­ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œå¤„ç†æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯ã€‚

### æ ¸å¿ƒç‰¹ç‚¹
- **è¯­ä¹‰è¡¨ç¤º**ï¼šç›¸ä¼¼å«ä¹‰çš„æ–‡æœ¬åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»æ›´è¿‘
- **é«˜ç»´ç©ºé—´**ï¼šé€šå¸¸ä½¿ç”¨å‡ ç™¾åˆ°å‡ åƒç»´çš„å‘é‡
- **æ•°å€¼è®¡ç®—**ï¼šæ”¯æŒæ•°å­¦è¿ç®—ï¼Œå¦‚ç›¸ä¼¼åº¦è®¡ç®—
- **é¢„è®­ç»ƒæ¨¡å‹**ï¼šåŸºäºå¤§è§„æ¨¡è¯­æ–™åº“è®­ç»ƒå¾—åˆ°

## ğŸ”§ æ ¸å¿ƒåŸç†

### 1. æ–‡æœ¬åˆ°å‘é‡çš„è½¬æ¢è¿‡ç¨‹

```mermaid
flowchart LR
    A["è¾“å…¥æ–‡æœ¬<br/>äººå·¥æ™ºèƒ½æŠ€æœ¯"] --> B["åˆ†è¯å¤„ç†<br/>äººå·¥|æ™ºèƒ½|æŠ€æœ¯"]
    B --> C["ç¼–ç å™¨<br/>Transformer"]
    C --> D["å‘é‡è¡¨ç¤º<br/>[0.1, -0.3, 0.8, ...]"] 
    D --> E["å½’ä¸€åŒ–<br/>L2 norm = 1"]
```

### 2. è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—

**ä½™å¼¦ç›¸ä¼¼åº¦å…¬å¼**ï¼š
```
cosine_similarity(A, B) = (A Â· B) / (||A|| Ã— ||B||)
```

- å€¼åŸŸï¼š[-1, 1]
- 1ï¼šå®Œå…¨ç›¸ä¼¼
- 0ï¼šæ— å…³
- -1ï¼šå®Œå…¨ç›¸å

### 3. å‘é‡ç©ºé—´ä¸­çš„è¯­ä¹‰å…³ç³»

```mermaid
graph TB
    subgraph "å‘é‡ç©ºé—´"
        A["äººå·¥æ™ºèƒ½<br/>[0.8, 0.6]"] 
        B["æœºå™¨å­¦ä¹ <br/>[0.7, 0.5]"]
        C["æ·±åº¦å­¦ä¹ <br/>[0.6, 0.4]"]
        D["å¤©æ°”é¢„æŠ¥<br/>[-0.2, 0.9]"]
        
        A -.ç›¸ä¼¼.-> B
        B -.ç›¸ä¼¼.-> C
        A -.ä¸ç›¸ä¼¼.-> D
    end
```

## ğŸ’» å…³é”®ä»£ç ç‰‡æ®µ

### 1. è·å–æ–‡æœ¬Embedding

```python
import dashscope
from dotenv import load_dotenv
import os

# é…ç½®API
load_dotenv()
dashscope.api_key = os.getenv('DASHSCOPE_API_KEY')

def get_embedding(text: str) -> List[float]:
    """è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º"""
    response = dashscope.TextEmbedding.call(
        model=dashscope.TextEmbedding.Models.text_embedding_v1,
        input=text
    )
    
    if response.status_code == 200:
        return response.output['embeddings'][0]['embedding']
    else:
        raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.message}")

# ä½¿ç”¨ç¤ºä¾‹
text = "äººå·¥æ™ºèƒ½æ˜¯æœªæ¥çš„å‘å±•æ–¹å‘"
embedding = get_embedding(text)
print(f"å‘é‡ç»´åº¦: {len(embedding)}")  # è¾“å‡º: 1536
```

### 2. è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦

```python
import numpy as np

def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    v1 = np.array(vec1)
    v2 = np.array(vec2)
    
    dot_product = np.dot(v1, v2)
    norm_v1 = np.linalg.norm(v1)
    norm_v2 = np.linalg.norm(v2)
    
    return dot_product / (norm_v1 * norm_v2)

# ä½¿ç”¨ç¤ºä¾‹
text1 = "äººå·¥æ™ºèƒ½æŠ€æœ¯"
text2 = "AIæŠ€æœ¯å‘å±•"

embedding1 = get_embedding(text1)
embedding2 = get_embedding(text2)

similarity = cosine_similarity(embedding1, embedding2)
print(f"ç›¸ä¼¼åº¦: {similarity:.4f}")  # è¾“å‡º: 0.8234
```

### 3. ç®€å•è¯­ä¹‰æœç´¢

```python
def find_most_similar(query: str, documents: List[str]) -> Tuple[str, float]:
    """åœ¨æ–‡æ¡£ä¸­æ‰¾åˆ°æœ€ç›¸ä¼¼çš„å†…å®¹"""
    query_embedding = get_embedding(query)
    
    best_doc = ""
    best_score = -1.0
    
    for doc in documents:
        doc_embedding = get_embedding(doc)
        similarity = cosine_similarity(query_embedding, doc_embedding)
        
        if similarity > best_score:
            best_score = similarity
            best_doc = doc
    
    return best_doc, best_score

# ä½¿ç”¨ç¤ºä¾‹
query = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
documents = [
    "æœºå™¨å­¦ä¹ æ˜¯AIçš„é‡è¦åˆ†æ”¯",
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½",
    "æ·±åº¦å­¦ä¹ ç®—æ³•å¾ˆå¤æ‚"
]

best_match, score = find_most_similar(query, documents)
print(f"æœ€ä½³åŒ¹é…: {best_match} (ç›¸ä¼¼åº¦: {score:.4f})")
```

## ğŸ”„ å¯¹æ¯”/ç±»æ¯”

### Embedding vs ä¼ ç»Ÿæ–‡æœ¬è¡¨ç¤º

| ç‰¹å¾ | ä¼ ç»Ÿæ–¹æ³•(å¦‚TF-IDF) | Embedding |
|------|-------------------|----------|
| **è¯­ä¹‰ç†è§£** | âŒ åŸºäºè¯é¢‘ï¼Œæ— è¯­ä¹‰ | âœ… ç†è§£è¯­ä¹‰å…³ç³» |
| **ç»´åº¦** | ç¨€ç–é«˜ç»´(è¯æ±‡è¡¨å¤§å°) | å¯†é›†ä½ç»´(é€šå¸¸1536) |
| **ç›¸ä¼¼åº¦** | è¯æ±‡é‡å  | è¯­ä¹‰ç›¸ä¼¼ |
| **æ³›åŒ–èƒ½åŠ›** | âŒ è¯æ±‡å¿…é¡»å®Œå…¨åŒ¹é… | âœ… ç†è§£åŒä¹‰è¯ã€è¿‘ä¹‰è¯ |
| **è®¡ç®—æ•ˆç‡** | å¿«é€Ÿ | éœ€è¦æ¨¡å‹æ¨ç† |

### ç±»æ¯”ç†è§£

**Embeddingå°±åƒç»™æ–‡æœ¬æ‹"è¯­ä¹‰ç…§ç‰‡"**ï¼š
- ğŸ“¸ **ç…§ç‰‡(å‘é‡)**ï¼šæ¯ä¸ªæ–‡æœ¬éƒ½æœ‰ä¸€å¼ ç‹¬ç‰¹çš„"è¯­ä¹‰ç…§ç‰‡"
- ğŸ¨ **é¢œè‰²(ç»´åº¦)**ï¼šç…§ç‰‡æœ‰RGBä¸‰ä¸ªé¢œè‰²é€šé“ï¼Œembeddingæœ‰1536ä¸ª"è¯­ä¹‰é€šé“"
- ğŸ“ **ç›¸ä¼¼åº¦**ï¼šä¸¤å¼ ç…§ç‰‡è¶Šç›¸ä¼¼ï¼Œå†…å®¹è¶Šæ¥è¿‘
- ğŸ” **æœç´¢**ï¼šé€šè¿‡æ¯”è¾ƒ"ç…§ç‰‡"æ‰¾åˆ°æœ€ç›¸ä¼¼çš„å†…å®¹

## âš ï¸ å¸¸è§å‘ & è§£å†³åŠæ³•

### 1. APIå¯†é’¥é…ç½®é—®é¢˜

**é—®é¢˜**ï¼š`Invalid API-key provided`

**è§£å†³åŠæ³•**ï¼š
```bash
# æ£€æŸ¥.envæ–‡ä»¶
cat .env | grep DASHSCOPE_API_KEY

# ç¡®ä¿æ ¼å¼æ­£ç¡®
DASHSCOPE_API_KEY=sk-your-actual-key-here

# é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡
source .env
```

### 2. å‘é‡ç»´åº¦ä¸åŒ¹é…

**é—®é¢˜**ï¼šä¸åŒæ¨¡å‹çš„embeddingç»´åº¦ä¸åŒ

**è§£å†³åŠæ³•**ï¼š
```python
# å§‹ç»ˆæ£€æŸ¥ç»´åº¦
embedding = get_embedding(text)
print(f"ç»´åº¦: {len(embedding)}")  # é€šä¹‰åƒé—®: 1536

# å¦‚æœéœ€è¦ï¼Œè¿›è¡Œç»´åº¦å¯¹é½
if len(embedding) != expected_dim:
    # æˆªæ–­æˆ–å¡«å……
    embedding = embedding[:expected_dim] + [0] * max(0, expected_dim - len(embedding))
```

### 3. æ–‡æœ¬é•¿åº¦é™åˆ¶

**é—®é¢˜**ï¼šè¾“å…¥æ–‡æœ¬è¿‡é•¿å¯¼è‡´APIè°ƒç”¨å¤±è´¥

**è§£å†³åŠæ³•**ï¼š
```python
def safe_get_embedding(text: str, max_length: int = 2000) -> List[float]:
    """å®‰å…¨è·å–embeddingï¼Œå¤„ç†é•¿æ–‡æœ¬"""
    if len(text) > max_length:
        # æˆªæ–­æ–‡æœ¬
        text = text[:max_length]
        print(f"âš ï¸  æ–‡æœ¬å·²æˆªæ–­åˆ°{max_length}å­—ç¬¦")
    
    return get_embedding(text)
```

### 4. æ‰¹é‡å¤„ç†æ•ˆç‡é—®é¢˜

**é—®é¢˜**ï¼šé€ä¸ªè°ƒç”¨APIæ•ˆç‡ä½

**è§£å†³åŠæ³•**ï¼š
```python
def batch_get_embeddings(texts: List[str], batch_size: int = 10) -> List[List[float]]:
    """æ‰¹é‡è·å–embeddings"""
    embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        
        # é€šä¹‰åƒé—®æ”¯æŒæ‰¹é‡è°ƒç”¨
        response = dashscope.TextEmbedding.call(
            model=dashscope.TextEmbedding.Models.text_embedding_v1,
            input=batch  # ä¼ å…¥åˆ—è¡¨
        )
        
        if response.status_code == 200:
            batch_embeddings = [emb['embedding'] for emb in response.output['embeddings']]
            embeddings.extend(batch_embeddings)
    
    return embeddings
```

## ğŸ“š æ¨èå»¶ä¼¸é˜…è¯»

1. **[é€šä¹‰åƒé—®Embedding APIæ–‡æ¡£](https://help.aliyun.com/zh/dashscope/developer-reference/text-embedding-api-details)**
   - å®˜æ–¹APIå‚è€ƒå’Œä½¿ç”¨ç¤ºä¾‹
   - æ”¯æŒçš„æ¨¡å‹å’Œå‚æ•°è¯´æ˜

2. **[Word2Vecåˆ°Transformerçš„æ¼”è¿›](https://jalammar.github.io/illustrated-word2vec/)**
   - æ–‡æœ¬embeddingæŠ€æœ¯çš„å‘å±•å†ç¨‹
   - ä»è¯å‘é‡åˆ°å¥å­å‘é‡çš„åŸç†

3. **[å‘é‡ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•å¯¹æ¯”](https://en.wikipedia.org/wiki/Cosine_similarity)**
   - ä½™å¼¦ç›¸ä¼¼åº¦ã€æ¬§å‡ é‡Œå¾—è·ç¦»ç­‰æ–¹æ³•
   - ä¸åŒåœºæ™¯ä¸‹çš„é€‰æ‹©å»ºè®®

---

## ğŸ¯ å­¦ä¹ æ£€æŸ¥æ¸…å•

å®Œæˆæœ¬èŠ‚å­¦ä¹ åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] ç†è§£ä»€ä¹ˆæ˜¯æ–‡æœ¬embeddingåŠå…¶ä½œç”¨
- [ ] æˆåŠŸè°ƒç”¨é€šä¹‰åƒé—®APIè·å–æ–‡æœ¬å‘é‡
- [ ] è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
- [ ] å®ç°ç®€å•çš„è¯­ä¹‰æœç´¢åŠŸèƒ½
- [ ] å¤„ç†å¸¸è§çš„APIè°ƒç”¨é—®é¢˜
- [ ] ç†è§£embeddingåœ¨RAGç³»ç»Ÿä¸­çš„é‡è¦æ€§

**ä¸‹ä¸€æ­¥**ï¼šå­¦ä¹ å¦‚ä½•ä½¿ç”¨FAISSå‘é‡æ•°æ®åº“é«˜æ•ˆå­˜å‚¨å’Œæ£€ç´¢å¤§é‡embeddingå‘é‡ï¼