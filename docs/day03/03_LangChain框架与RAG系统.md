# 03_LangChainæ¡†æ¶ä¸RAGç³»ç»Ÿ

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### LangChainæ¡†æ¶
LangChainæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)åº”ç”¨çš„å¼€æºæ¡†æ¶ï¼Œå®ƒæä¾›äº†ä¸°å¯Œçš„ç»„ä»¶å’Œå·¥å…·ï¼Œç®€åŒ–äº†AIåº”ç”¨çš„å¼€å‘è¿‡ç¨‹ã€‚

### RAGç³»ç»Ÿ (Retrieval-Augmented Generation)
RAGæ˜¯ä¸€ç§ç»“åˆä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆçš„AIæŠ€æœ¯ï¼Œé€šè¿‡æ£€ç´¢ç›¸å…³çŸ¥è¯†æ¥å¢å¼ºç”Ÿæˆæ¨¡å‹çš„å›ç­”è´¨é‡ã€‚

## ğŸ”§ æ ¸å¿ƒç»„ä»¶

### 1. LangChainæ ¸å¿ƒç»„ä»¶

```mermaid
graph TD
    A[LangChain æ¡†æ¶]

    subgraph æ–‡æ¡£åŠ è½½å™¨
        B1[TextLoader]
        B2[PDFLoader]
        B3[WebLoader]
    end

    subgraph æ–‡æœ¬åˆ†å‰²å™¨
        C1[RecursiveCharacterTextSplitter]
        C2[TokenTextSplitter]
    end

    subgraph å‘é‡å­˜å‚¨
        D1[ChromaDB]
        D2[FAISS]
        D3[Pinecone]
    end

    subgraph æ£€ç´¢å™¨
        E1[VectorStoreRetriever]
    end

    subgraph LLMåŒ…è£…å™¨
        F1[ChatOpenAI]
        F2[Qwen Chat API]
    end

    subgraph æç¤ºæ¨¡æ¿
        G1[PromptTemplate]
    end

    subgraph é“¾
        H1[LLMChain]
        H2[RetrievalQA]
        H3[ConversationalRetrievalChain]
    end

    A --> æ–‡æ¡£åŠ è½½å™¨
    A --> æ–‡æœ¬åˆ†å‰²å™¨
    A --> å‘é‡å­˜å‚¨
    A --> æ£€ç´¢å™¨
    A --> LLMåŒ…è£…å™¨
    A --> æç¤ºæ¨¡æ¿
    A --> é“¾

    æ–‡æ¡£åŠ è½½å™¨ --> æ–‡æœ¬åˆ†å‰²å™¨
    æ–‡æœ¬åˆ†å‰²å™¨ --> å‘é‡å­˜å‚¨
    å‘é‡å­˜å‚¨ --> æ£€ç´¢å™¨
    æ£€ç´¢å™¨ --> é“¾
    LLMåŒ…è£…å™¨ --> é“¾
    æç¤ºæ¨¡æ¿ --> é“¾

```

### 2. RAGç³»ç»Ÿæ¶æ„

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant R as RAGç³»ç»Ÿ
    participant V as å‘é‡æ•°æ®åº“
    participant L as LLM
    
    U->>R: æå‡ºé—®é¢˜
    R->>R: é—®é¢˜å‘é‡åŒ–
    R->>V: æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£
    V->>R: è¿”å›ç›¸å…³æ–‡æ¡£
    R->>R: æ„å»ºä¸Šä¸‹æ–‡
    R->>L: å‘é€æç¤º+ä¸Šä¸‹æ–‡
    L->>R: ç”Ÿæˆå›ç­”
    R->>U: è¿”å›æœ€ç»ˆç­”æ¡ˆ
```

## ğŸ’» å…³é”®ä»£ç ç‰‡æ®µ

### 1. LangChainæ–‡æœ¬åˆ†å‰²å™¨

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,          # æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°
    chunk_overlap=50,        # å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ";", "ï¼Œ", " ", ""]
)

# åˆ†å‰²æ–‡æœ¬
chunks = text_splitter.split_text(long_text)
```

### 2. è‡ªå®šä¹‰LLMåŒ…è£…å™¨

```python
from langchain_community.llms.base import LLM
from typing import Optional, List

class QwenLLM(LLM):
    """é€šä¹‰åƒé—®LLMåŒ…è£…å™¨"""
    
    @property
    def _llm_type(self) -> str:
        return "qwen"
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        # è°ƒç”¨é€šä¹‰åƒé—®API
        response = Generation.call(
            model="qwen-turbo",
            prompt=prompt,
            max_tokens=1000,
            temperature=0.7
        )
        return response.output.text.strip()
```

### 3. RAGæç¤ºæ¨¡æ¿

```python
from langchain.prompts import PromptTemplate

prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="""
åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´æ˜æ— æ³•ä»æä¾›çš„ä¿¡æ¯ä¸­æ‰¾åˆ°ç­”æ¡ˆã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†å’Œå‡†ç¡®çš„å›ç­”ï¼š
"""
)
```

### 4. å®Œæ•´RAGæŸ¥è¯¢æµç¨‹

```python
def query(self, question: str, top_k: int = 3) -> Dict[str, Any]:
    """å®Œæ•´çš„RAGæŸ¥è¯¢æµç¨‹"""
    # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
    retrieved_docs = self.retrieve_documents(question, top_k=top_k)
    
    # 2. æ„å»ºä¸Šä¸‹æ–‡
    context_parts = []
    for i, doc in enumerate(retrieved_docs, 1):
        context_parts.append(f"æ–‡æ¡£{i}: {doc['document']}")
    context = "\n\n".join(context_parts)
    
    # 3. ç”Ÿæˆæç¤º
    prompt = self.prompt_template.format(context=context, question=question)
    
    # 4. è°ƒç”¨LLMç”Ÿæˆå›ç­”
    answer = self.llm._call(prompt)
    
    return {
        'question': question,
        'answer': answer,
        'retrieved_docs': retrieved_docs,
        'retrieval_time': retrieval_time,
        'generation_time': generation_time,
        'total_time': total_time
    }
```

## ğŸ†š å¯¹æ¯”åˆ†æ

### RAG vs ä¼ ç»Ÿé—®ç­”ç³»ç»Ÿ

| ç‰¹æ€§ | ä¼ ç»Ÿé—®ç­” | RAGç³»ç»Ÿ |
|------|----------|----------|
| çŸ¥è¯†æ¥æº | æ¨¡å‹å‚æ•° | å¤–éƒ¨çŸ¥è¯†åº“ |
| çŸ¥è¯†æ›´æ–° | éœ€è¦é‡è®­ç»ƒ | å®æ—¶æ›´æ–°çŸ¥è¯†åº“ |
| å›ç­”å‡†ç¡®æ€§ | ä¾èµ–è®­ç»ƒæ•°æ® | åŸºäºæ£€ç´¢åˆ°çš„äº‹å® |
| å¯è§£é‡Šæ€§ | è¾ƒä½ | å¯è¿½æº¯ä¿¡æ¯æº |
| è®¡ç®—æˆæœ¬ | è¾ƒä½ | è¾ƒé«˜(æ£€ç´¢+ç”Ÿæˆ) |
| é¢†åŸŸé€‚åº”æ€§ | éœ€è¦å¾®è°ƒ | æ›´æ¢çŸ¥è¯†åº“å³å¯ |

### LangChain vs è‡ªå»ºæ¡†æ¶

| æ–¹é¢ | è‡ªå»ºæ¡†æ¶ | LangChain |
|------|----------|----------|
| å¼€å‘é€Ÿåº¦ | æ…¢ | å¿« |
| ç»„ä»¶ä¸°å¯Œåº¦ | æœ‰é™ | ä¸°å¯Œ |
| ç¤¾åŒºæ”¯æŒ | æ—  | æ´»è·ƒ |
| å­¦ä¹ æˆæœ¬ | é«˜ | ä¸­ç­‰ |
| å®šåˆ¶åŒ–ç¨‹åº¦ | é«˜ | ä¸­ç­‰ |
| ç»´æŠ¤æˆæœ¬ | é«˜ | ä½ |

## âš ï¸ å¸¸è§å‘ç‚¹ä¸è§£å†³æ–¹æ¡ˆ

### 1. æ–‡æœ¬åˆ†å‰²é—®é¢˜

**é—®é¢˜**: åˆ†å‰²åçš„æ–‡æœ¬å—è¯­ä¹‰ä¸å®Œæ•´

**è§£å†³æ–¹æ¡ˆ**:
```python
# ä½¿ç”¨åˆé€‚çš„åˆ†éš”ç¬¦å’Œé‡å 
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,  # é‡è¦ï¼šä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ"]  # æŒ‰è¯­ä¹‰è¾¹ç•Œåˆ†å‰²
)
```

### 2. æ£€ç´¢è´¨é‡é—®é¢˜

**é—®é¢˜**: æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸é—®é¢˜ä¸ç›¸å…³

**è§£å†³æ–¹æ¡ˆ**:
- ä¼˜åŒ–embeddingæ¨¡å‹é€‰æ‹©
- è°ƒæ•´æ£€ç´¢å‚æ•°(top_k, similarity_threshold)
- ä½¿ç”¨é‡æ’åº(reranking)æŠ€æœ¯
- æ”¹è¿›æŸ¥è¯¢é¢„å¤„ç†

### 3. ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶

**é—®é¢˜**: æ£€ç´¢åˆ°çš„æ–‡æ¡£è¶…è¿‡LLMä¸Šä¸‹æ–‡é•¿åº¦

**è§£å†³æ–¹æ¡ˆ**:
```python
def truncate_context(docs, max_length=2000):
    """æˆªæ–­ä¸Šä¸‹æ–‡ä»¥é€‚åº”LLMé™åˆ¶"""
    context = ""
    for doc in docs:
        if len(context + doc['document']) > max_length:
            break
        context += doc['document'] + "\n\n"
    return context
```

### 4. æ€§èƒ½ä¼˜åŒ–é—®é¢˜

**é—®é¢˜**: æŸ¥è¯¢å“åº”æ—¶é—´è¿‡é•¿

**è§£å†³æ–¹æ¡ˆ**:
- ä½¿ç”¨å‘é‡æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–
- å®ç°å¼‚æ­¥å¤„ç†
- ç¼“å­˜å¸¸è§æŸ¥è¯¢ç»“æœ
- å¹¶è¡Œå¤„ç†æ£€ç´¢å’Œç”Ÿæˆ

## ğŸ“– æ¨èå»¶ä¼¸é˜…è¯»

1. **LangChainå®˜æ–¹æ–‡æ¡£**: [https://python.langchain.com/docs/get_started/introduction](https://python.langchain.com/docs/get_started/introduction)
   - å…¨é¢çš„æ¡†æ¶ä»‹ç»å’Œä½¿ç”¨æŒ‡å—

2. **RAGæŠ€æœ¯è®ºæ–‡**: "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
   - RAGæŠ€æœ¯çš„åŸç†å’Œå®ç°ç»†èŠ‚

3. **é€šä¹‰åƒé—®APIæ–‡æ¡£**: [https://help.aliyun.com/zh/dashscope/](https://help.aliyun.com/zh/dashscope/)
   - APIä½¿ç”¨æ–¹æ³•å’Œæœ€ä½³å®è·µ

## âœ… å­¦ä¹ æ£€æŸ¥æ¸…å•

- [ ] ç†è§£LangChainæ¡†æ¶çš„æ ¸å¿ƒç»„ä»¶
- [ ] æŒæ¡æ–‡æ¡£åŠ è½½å’Œæ–‡æœ¬åˆ†å‰²æŠ€æœ¯
- [ ] å®ç°è‡ªå®šä¹‰LLMåŒ…è£…å™¨
- [ ] æ„å»ºå®Œæ•´çš„RAGæŸ¥è¯¢æµç¨‹
- [ ] ç†è§£æç¤ºå·¥ç¨‹çš„é‡è¦æ€§
- [ ] æŒæ¡æ€§èƒ½ä¼˜åŒ–æŠ€å·§
- [ ] äº†è§£å¸¸è§é—®é¢˜çš„è§£å†³æ–¹æ¡ˆ

## ğŸ¯ å®è·µæŠ€å·§

### 1. æç¤ºå·¥ç¨‹æœ€ä½³å®è·µ

```python
# å¥½çš„æç¤ºæ¨¡æ¿
template = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

é‡è¦è§„åˆ™ï¼š
1. åªåŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”
2. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œæ˜ç¡®è¯´æ˜
3. ä¿æŒå›ç­”ç®€æ´å‡†ç¡®
4. å¯ä»¥å¼•ç”¨å…·ä½“çš„æ–‡æ¡£ç‰‡æ®µ

ä¸Šä¸‹æ–‡ï¼š{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š
"""
```

### 2. å…ƒæ•°æ®ç®¡ç†

```python
# ä¸ºæ–‡æ¡£æ·»åŠ ä¸°å¯Œçš„å…ƒæ•°æ®
metadata = {
    'source': 'document.pdf',
    'page': 1,
    'section': '2.1',
    'topic': 'AIåŸºç¡€',
    'difficulty': 'beginner',
    'last_updated': '2024-01-15',
    'author': 'Expert Name'
}
```

### 3. æŸ¥è¯¢ä¼˜åŒ–

```python
def preprocess_query(query: str) -> str:
    """æŸ¥è¯¢é¢„å¤„ç†"""
    # ç§»é™¤åœç”¨è¯
    # åŒä¹‰è¯æ‰©å±•
    # æ‹¼å†™çº æ­£
    return processed_query

def postprocess_results(results: List[Dict]) -> List[Dict]:
    """ç»“æœåå¤„ç†"""
    # å»é‡
    # é‡æ’åº
    # è¿‡æ»¤ä½è´¨é‡ç»“æœ
    return filtered_results
```

### 4. é”™è¯¯å¤„ç†

```python
def robust_query(self, question: str, max_retries: int = 3):
    """å¸¦é‡è¯•æœºåˆ¶çš„æŸ¥è¯¢"""
    for attempt in range(max_retries):
        try:
            return self.query(question)
        except Exception as e:
            if attempt == max_retries - 1:
                return self._fallback_response(question, str(e))
            time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
```

## ğŸ”„ ä¸å‰é¢çŸ¥è¯†çš„è”ç³»

- **Day 1 Embedding**: RAGç³»ç»Ÿçš„æ£€ç´¢åŸºç¡€
- **Day 2 å‘é‡æ•°æ®åº“**: RAGç³»ç»Ÿçš„å­˜å‚¨åç«¯
- **Day 3 LangChain**: æ•´åˆå‰é¢æŠ€æœ¯çš„æ¡†æ¶

## ğŸ¯ ä¸‹ä¸€æ­¥å­¦ä¹ æ–¹å‘

- **é«˜çº§RAGæŠ€æœ¯**: å¤šè½®å¯¹è¯ã€é‡æ’åºã€æ··åˆæ£€ç´¢
- **Agentç³»ç»Ÿ**: åŸºäºLangChainæ„å»ºæ™ºèƒ½ä»£ç†
- **ç”Ÿäº§éƒ¨ç½²**: æ€§èƒ½ä¼˜åŒ–ã€ç›‘æ§ã€æ‰©å±•æ€§